{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword-Analyse Tool für SEO\n",
    "\n",
    "Dieses Notebook analysiert eine Webseite und identifiziert Keywords, die für Google relevant sein könnten. Dabei werden statistische Methoden wie TF-IDF, RAKE und HTML-Gewichtung kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:14:23.101057Z",
     "iopub.status.busy": "2026-02-17T13:14:23.100963Z",
     "iopub.status.idle": "2026-02-17T13:14:46.901101Z",
     "shell.execute_reply": "2026-02-17T13:14:46.899475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ayyouboss/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/ayyouboss/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ayyouboss/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK Daten herunterladen\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web-Scraping & Datenextraktion\n",
    "\n",
    "Zuerst laden wir den Inhalt der Seite herunter und extrahieren Text aus wichtigen SEO-Elementen (Title, H1, H2, Body)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:14:46.903008Z",
     "iopub.status.busy": "2026-02-17T13:14:46.902810Z",
     "iopub.status.idle": "2026-02-17T13:14:46.906596Z",
     "shell.execute_reply": "2026-02-17T13:14:46.905896Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # SEO Relevante Elemente extrahieren\n",
    "    data = {\n",
    "        'title': soup.title.string if soup.title else '',\n",
    "        'h1': [h1.get_text().strip() for h1 in soup.find_all('h1')],\n",
    "        'h2': [h2.get_text().strip() for h2 in soup.find_all('h2')],\n",
    "        'meta_description': '',\n",
    "        'body_text': ''\n",
    "    }\n",
    "    \n",
    "    desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if desc:\n",
    "        data['meta_description'] = desc.get('content', '')\n",
    "    \n",
    "    # Script und Style Tags entfernen\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "        \n",
    "    data['body_text'] = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text-Vorverarbeitung\n",
    "\n",
    "Reinigung des Textes und Vorbereitung für die statistische Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:14:46.908137Z",
     "iopub.status.busy": "2026-02-17T13:14:46.907948Z",
     "iopub.status.idle": "2026-02-17T13:14:46.911029Z",
     "shell.execute_reply": "2026-02-17T13:14:46.910539Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text, lang='german'):\n",
    "    # Kleinschreibung und Sonderzeichen entfernen\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zäöüß\\s]', '', text)\n",
    "    \n",
    "    # Stopwords entfernen\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    words = text.split()\n",
    "    cleaned_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    \n",
    "    return \" \".join(cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistische Analyse & Gewichtung\n",
    "\n",
    "Hier kombinieren wir verschiedene Scores:\n",
    "- **TF-IDF**: Wie wichtig ist das Wort im Text?\n",
    "- **HTML-Boost**: Ist das Wort in einer Überschrift oder im Titel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:14:46.912608Z",
     "iopub.status.busy": "2026-02-17T13:14:46.912375Z",
     "iopub.status.idle": "2026-02-17T13:14:46.916011Z",
     "shell.execute_reply": "2026-02-17T13:14:46.915521Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keywords(url, lang='german'):\n",
    "    raw_data = scrape_website(url)\n",
    "    full_text = f\"{raw_data['title']} {raw_data['meta_description']} {' '.join(raw_data['h1'])} {' '.join(raw_data['h2'])} {raw_data['body_text']}\"\n",
    "    \n",
    "    cleaned_text = clean_text(full_text, lang)\n",
    "    \n",
    "    # TF-IDF Analyse (für Einzelwörter)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "    tfidf_matrix = vectorizer.fit_transform([cleaned_text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "    \n",
    "    word_scores = dict(zip(feature_names, scores))\n",
    "    \n",
    "    # RAKE für Phrasen\n",
    "    r = Rake(language=lang)\n",
    "    r.extract_keywords_from_text(full_text)\n",
    "    rake_phrases = r.get_ranked_phrases_with_scores()[:20]\n",
    "    \n",
    "    # SEO Boost-System\n",
    "    # Wir geben Wörtern in Title und H1 einen höheren Score\n",
    "    boosted_scores = word_scores.copy()\n",
    "    \n",
    "    important_areas = {\n",
    "        'title': 5.0,\n",
    "        'h1': 3.0,\n",
    "        'h2': 1.5,\n",
    "        'meta_description': 2.0\n",
    "    }\n",
    "    \n",
    "    for area, boost in important_areas.items():\n",
    "        content = \"\"\n",
    "        if isinstance(raw_data[area], list):\n",
    "            content = \" \".join(raw_data[area])\n",
    "        else:\n",
    "            content = raw_data[area]\n",
    "        \n",
    "        area_words = clean_text(content, lang).split()\n",
    "        for word in area_words:\n",
    "            if word in boosted_scores:\n",
    "                boosted_scores[word] *= boost\n",
    "    \n",
    "    # Ergebnisse aufbereiten\n",
    "    sorted_keywords = sorted(boosted_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_keywords[:20], rake_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testlauf\n",
    "\n",
    "Geben Sie hier die URL ein, die Sie analysieren möchten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T13:14:46.918386Z",
     "iopub.status.busy": "2026-02-17T13:14:46.918293Z",
     "iopub.status.idle": "2026-02-17T13:14:47.157175Z",
     "shell.execute_reply": "2026-02-17T13:14:47.156815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top Keywords für https://www.google.com ---\n",
      "                Keyword     Score\n",
      "0                google  3.441236\n",
      "1              anmelden  0.229416\n",
      "2                bilder  0.229416\n",
      "3  datenschutzerklärung  0.229416\n",
      "4            erweiterte  0.229416\n",
      "5                 gmail  0.229416\n",
      "6              googlede  0.229416\n",
      "7   nutzungsbedingungen  0.229416\n",
      "8                 suche  0.229416\n",
      "9  unternehmensangebote  0.229416\n",
      "\n",
      "--- Top Phrasen (RAKE) ---\n",
      "google google gmail bilder anmelden erweiterte suche werbeprogramme unternehmensangebote (Score: 74.00)\n",
      "google google (Score: 11.00)\n",
      "de © 2026 (Score: 9.00)\n",
      "nutzungsbedingungen (Score: 1.00)\n",
      "datenschutzerklärung (Score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "URL_TO_ANALYZE = \"https://www.google.com\" # Beispiel-URL\n",
    "keywords, phrases = get_keywords(URL_TO_ANALYZE, 'german')\n",
    "\n",
    "print(f\"--- Top Keywords für {URL_TO_ANALYZE} ---\")\n",
    "df_kw = pd.DataFrame(keywords, columns=['Keyword', 'Score'])\n",
    "print(df_kw.head(10))\n",
    "\n",
    "print(\"\\n--- Top Phrasen (RAKE) ---\")\n",
    "for score, phrase in phrases[:10]:\n",
    "    print(f\"{phrase} (Score: {score:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
